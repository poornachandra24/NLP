{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Tutorial: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text ,\" | \",ent.label_,\" | \",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List down all the entities\n",
    "List of entities are also documented on this page: https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok2vec': [],\n",
       " 'tagger': ['$',\n",
       "  \"''\",\n",
       "  ',',\n",
       "  '-LRB-',\n",
       "  '-RRB-',\n",
       "  '.',\n",
       "  ':',\n",
       "  'ADD',\n",
       "  'AFX',\n",
       "  'CC',\n",
       "  'CD',\n",
       "  'DT',\n",
       "  'EX',\n",
       "  'FW',\n",
       "  'HYPH',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'JJR',\n",
       "  'JJS',\n",
       "  'LS',\n",
       "  'MD',\n",
       "  'NFP',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'NNPS',\n",
       "  'NNS',\n",
       "  'PDT',\n",
       "  'POS',\n",
       "  'PRP',\n",
       "  'PRP$',\n",
       "  'RB',\n",
       "  'RBR',\n",
       "  'RBS',\n",
       "  'RP',\n",
       "  'SYM',\n",
       "  'TO',\n",
       "  'UH',\n",
       "  'VB',\n",
       "  'VBD',\n",
       "  'VBG',\n",
       "  'VBN',\n",
       "  'VBP',\n",
       "  'VBZ',\n",
       "  'WDT',\n",
       "  'WP',\n",
       "  'WP$',\n",
       "  'WRB',\n",
       "  'XX',\n",
       "  '_SP',\n",
       "  '``'],\n",
       " 'parser': ['ROOT',\n",
       "  'acl',\n",
       "  'acomp',\n",
       "  'advcl',\n",
       "  'advmod',\n",
       "  'agent',\n",
       "  'amod',\n",
       "  'appos',\n",
       "  'attr',\n",
       "  'aux',\n",
       "  'auxpass',\n",
       "  'case',\n",
       "  'cc',\n",
       "  'ccomp',\n",
       "  'compound',\n",
       "  'conj',\n",
       "  'csubj',\n",
       "  'csubjpass',\n",
       "  'dative',\n",
       "  'dep',\n",
       "  'det',\n",
       "  'dobj',\n",
       "  'expl',\n",
       "  'intj',\n",
       "  'mark',\n",
       "  'meta',\n",
       "  'neg',\n",
       "  'nmod',\n",
       "  'npadvmod',\n",
       "  'nsubj',\n",
       "  'nsubjpass',\n",
       "  'nummod',\n",
       "  'oprd',\n",
       "  'parataxis',\n",
       "  'pcomp',\n",
       "  'pobj',\n",
       "  'poss',\n",
       "  'preconj',\n",
       "  'predet',\n",
       "  'prep',\n",
       "  'prt',\n",
       "  'punct',\n",
       "  'quantmod',\n",
       "  'relcl',\n",
       "  'xcomp'],\n",
       " 'attribute_ruler': [],\n",
       " 'lemmatizer': [],\n",
       " 'ner': ['CARDINAL',\n",
       "  'DATE',\n",
       "  'EVENT',\n",
       "  'FAC',\n",
       "  'GPE',\n",
       "  'LANGUAGE',\n",
       "  'LAW',\n",
       "  'LOC',\n",
       "  'MONEY',\n",
       "  'NORP',\n",
       "  'ORDINAL',\n",
       "  'ORG',\n",
       "  'PERCENT',\n",
       "  'PERSON',\n",
       "  'PRODUCT',\n",
       "  'QUANTITY',\n",
       "  'TIME',\n",
       "  'WORK_OF_ART']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prints all the entities that spcay supports with this particular model\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg | PERSON | People, including fictional\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above it made a mistake in identifying Bloomberg the company. Let's try hugging face for this now.\n",
    "\n",
    "https://huggingface.co/dslim/bert-base-NER?text=Michael+Bloomberg+founded+Bloomberg+in+1982\n",
    "\n",
    "Here also go through 3 sample examples for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  0 | 9\n",
      "Twitter Inc  |  ORG  |  30 | 41\n",
      "$45 billion  |  MONEY  |  46 | 57\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting custom entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  PRODUCT\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going to acquire"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER): Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise: 1\n",
    "Extract all the Geographical (cities, Countries, states) names from a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Kiran want to know the famous foods in each state of India. So, he opened Google and search for this question. Google showed that\n",
    "in Delhi it is Chaat, in Gujarat it is Dal Dhokli, in Tamilnadu it is Pongal, in Andhrapradesh it is Biryani, in Assam it is Papaya Khar,\n",
    "in Bihar it is Litti Chowkha and so on for all other states\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiran | ORG | Companies, agencies, institutions, etc.\n",
      "India | GPE | Countries, cities, states\n",
      "Google | ORG | Companies, agencies, institutions, etc.\n",
      "Google | ORG | Companies, agencies, institutions, etc.\n",
      "Delhi | GPE | Countries, cities, states\n",
      "Chaat | ORG | Companies, agencies, institutions, etc.\n",
      "Gujarat | GPE | Countries, cities, states\n",
      "Dal Dhokli | PERSON | People, including fictional\n",
      "Tamilnadu | GPE | Countries, cities, states\n",
      "Pongal | GPE | Countries, cities, states\n",
      "Andhrapradesh | GPE | Countries, cities, states\n",
      "Biryani | PERSON | People, including fictional\n",
      "Assam | GPE | Countries, cities, states\n",
      "Papaya Khar | PERSON | People, including fictional\n",
      "Bihar | GPE | Countries, cities, states\n",
      "Litti Chowkha | PERSON | People, including fictional\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text ,\"|\", ent.label_,\"|\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pongal"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[45:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India | GPE | Countries, cities, states\n",
      "Delhi | GPE | Countries, cities, states\n",
      "Gujarat | GPE | Countries, cities, states\n",
      "Tamilnadu | GPE | Countries, cities, states\n",
      "Andhrapradesh | GPE | Countries, cities, states\n",
      "Assam | GPE | Countries, cities, states\n",
      "Bihar | GPE | Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "#pongal is food vut is labelled as GPE\n",
    "s1 = Span(doc, 45, 46, label=\"FOOD\")\n",
    "doc.set_ents([s1], default=\"unmodified\")\n",
    "\n",
    "geographical_locations=[]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_=='GPE':#checking the whether token belongs to entity \"GPE\" [Geographical location]\n",
    "        print(ent.text ,\"|\", ent.label_,\"|\",spacy.explain(ent.label_))\n",
    "        geographical_locations.append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'Delhi', 'Gujarat', 'Tamilnadu', 'Andhrapradesh', 'Assam', 'Bihar']\n",
      "count= 7\n"
     ]
    }
   ],
   "source": [
    "print(geographical_locations)\n",
    "print('count=',len(geographical_locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise: 2\n",
    "Extract all the birth dates of cricketers in the given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Sachin Tendulkar was born on 24 April 1973, Virat Kholi was born on 5 November 1988, Dhoni was born on 7 July 1981\n",
    "and finally Ricky ponting was born on 19 December 1974.\"\"\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin Tendulkar | PERSON | People, including fictional\n",
      "24 April 1973 | DATE | Absolute or relative dates or periods\n",
      "Virat Kholi | LOC | Non-GPE locations, mountain ranges, bodies of water\n",
      "5 November 1988 | DATE | Absolute or relative dates or periods\n",
      "Dhoni | PERSON | People, including fictional\n",
      "7 July 1981 | DATE | Absolute or relative dates or periods\n",
      "Ricky | PERSON | People, including fictional\n",
      "19 December 1974 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,\"|\",ent.label_,\"|\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24 April 1973', '5 November 1988', '7 July 1981', '19 December 1974']\n",
      "count = 4\n"
     ]
    }
   ],
   "source": [
    "Dates=[]\n",
    "for ent in doc.ents:\n",
    "    if (ent.label_==\"DATE\"): #checking the whether token belongs to entity \"DATE\" [Dates]\n",
    "        Dates.append(ent.text)\n",
    "print(Dates)\n",
    "print(\"count =\",len(Dates))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
